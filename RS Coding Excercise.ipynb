{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Excercise: Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: implement a simple item-based KNN recommendation + content-based mapping algorithm to recommend top-2 items to users (for example, recommend another 2 items for user1). You can use any language. Consider to use proper data structures to make the algorithm efficient for large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0512 16:42:13.110000 13428 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\AncillaryRS\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 16:42:25.166000 13428 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\envs\\AncillaryRS\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that purchase history of users is represented as item lists of variable length (depending on the number of purchases), normalise the data format and convert it to a long Pandas data frame.<br><br>\n",
    "User1: item1, item3<br>\n",
    "User2: item1, item3<br>\n",
    "User3: item2, item4<br>\n",
    "...<br>\n",
    "*Useri: item1, item2, itemj, itemk*<br><br>\n",
    "**Please let me know if the above assumption is wrong and the data always contains pairs of products (temporally directed graph?)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {'User1': ['item1', 'item3'],\n",
    "             'User2': ['item1', 'item3'],\n",
    "             'User3': ['item2', 'item4']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User1</td>\n",
       "      <td>item1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User1</td>\n",
       "      <td>item3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User2</td>\n",
       "      <td>item1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User2</td>\n",
       "      <td>item3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User3</td>\n",
       "      <td>item2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  UserID ItemID\n",
       "0  User1  item1\n",
       "1  User1  item3\n",
       "2  User2  item1\n",
       "3  User2  item3\n",
       "4  User3  item2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for user in input_data:\n",
    "    for item in input_data[user]:\n",
    "        line = [user, item]\n",
    "        dataset.append(line)\n",
    "\n",
    "dataset = pd.DataFrame(dataset, columns=['UserID', 'ItemID'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Based Collaborative Filtering\n",
    "### Simple bruteforce approach (suitable if dataset is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The purchase data is likely to be sparse. Cosine similarity between two binary vectors can ne defined as \n",
    "$$S(_{A,B} = \\frac{c}{\\sqrt{ab}})$$\n",
    "where:<br> a = number of occurrences for item A<br>\n",
    "b = number of occurrences for item B<br>\n",
    "c = number of shared occurrences for both items (purchased by the same user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of occurrences for all items\n",
    "occurrences = dataset.groupby(['ItemID']).count()\n",
    "\n",
    "# Initialise cache\n",
    "collab_cache = pd.DataFrame(columns=['Previously purchased item', 'Recommended item', 'Cosine Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_bin(item):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between the item and all other items based on user purchasing patterns\n",
    "    \"\"\"\n",
    "\n",
    "    a = occurrences.loc[item][0]\n",
    "    b = occurrences.values[:,0]\n",
    "    vector_A = dataset[dataset['ItemID']==item]\n",
    "    vector_B = dataset\n",
    "    overlap = pd.merge(vector_A, vector_B, how='inner', on=['UserID']).groupby(['ItemID_y']).count()['UserID']\n",
    "    c = pd.concat([occurrences, overlap], axis=1, sort=True).fillna(0).iloc[:,1]\n",
    "    S = c / np.sqrt(a*b)\n",
    "    \n",
    "    return pd.concat([pd.Series(np.resize(item, len(S))), pd.Series(S.index), \n",
    "                      pd.Series(S.values)], axis=1, sort=False).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruteforce_recs(user, sim_type, repeat_rec=False, caching=True, n=2):\n",
    "    \"\"\"\n",
    "    Given user ID and type of recommendation algorithm required \n",
    "    returns averaged item cosine similarities for all possible recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    previous_purchases = dataset[dataset['UserID']==user]['ItemID']\n",
    "    recommendations = []\n",
    "    \n",
    "    if sim_type=='semantic':\n",
    "        unique_items = reviews['ItemID'].unique()\n",
    "        cos_sim = cosine_similarity_semantic\n",
    "        cache = review_cache\n",
    "    elif sim_type=='collaborative':\n",
    "        unique_items = dataset['ItemID'].unique()\n",
    "        cos_sim = cosine_similarity_bin\n",
    "        cache = collab_cache\n",
    "    else:\n",
    "        return print('Please use valid sim_type argument: \"semantic\" or \"collaborative\"')\n",
    "     \n",
    "    for i in previous_purchases:\n",
    "        rec_retrieved = False\n",
    "        if (caching == True) & (len(cache) > 0):\n",
    "            if (i in cache['Previously purchased item'].unique()):\n",
    "                recommendations.extend(cache[cache['Previously purchased item']==i].values)\n",
    "                rec_retrieved = True\n",
    "        if not rec_retrieved:\n",
    "            S = cos_sim(i)  \n",
    "            recommendations.extend(S)\n",
    "            if (caching == True):\n",
    "                for row in S:\n",
    "                    cache.loc[len(cache),:] = row\n",
    "     \n",
    "    recommendations = pd.DataFrame(recommendations, columns=['Previously purchased item',\n",
    "                                                                 'Recommended item', 'Cosine Similarity'])\n",
    "    \n",
    "    if repeat_rec == False:\n",
    "        allowed_items = [x for x in unique_items if x not in previous_purchases.values]\n",
    "        recommendations = recommendations[recommendations['Recommended item'].isin(allowed_items)].reset_index(drop = True)\n",
    "    \n",
    "    return recommendations.groupby(['Recommended item']).mean().sort_values(by='Cosine Similarity', \n",
    "                                                                            ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two best recommendations are returned for *User1* based on simple item-to-item collaborative filtering approach. This is based on the assumption that we don't intend to recommend to repurchase the same items again, as is probably the case with movies, however, this parameter can be easily changed by passing *'repeat_rec=True'* to the function above.<br><br>\n",
    "In the specific case below, there are only two available items that can be recommended to *User1*, and both of them maximally dissimilar to the purchase history of this user, as there is no purchase history overlap between users in the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recommended item</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Cosine Similarity\n",
       "Recommended item                   \n",
       "item2                           0.0\n",
       "item4                           0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bruteforce_recs('User1', 'collaborative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix factorisation (suitable for large datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alternative to simple KNN search, [Alternating Least Squares (ALS)](http://yifanhu.net/PUB/cf.pdf) (Hu, Koren & Volinsky, 2008; Takács, Pilászy & Tikk, 2011) matrix factorisation model is often used for binary user ratings (e.g., purchase history or click counts) on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 1, 0],\n",
       "        [0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformat data as a sparse matrix\n",
    "dense_df = pd.DataFrame()\n",
    "for col in dataset.columns:\n",
    "    dense_df[col] = dataset[col].astype(\"category\")\n",
    "    dense_df[col+'_code'] = dense_df[col].cat.codes\n",
    "dense_df['Counts'] = 1 # assumin user-item value pairs are unique in the initial dataset\n",
    "\n",
    "sparse_item_user = sparse.csr_matrix((dense_df['Counts'], (dense_df['ItemID_code'], dense_df['UserID_code'])))\n",
    "sparse_user_item = sparse.csr_matrix((dense_df['Counts'], (dense_df['UserID_code'], dense_df['ItemID_code'])))\n",
    "sparse_item_user.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 16:46:56.225999 13428 utils.py:29] Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "100%|████████████████████████████████████████| 10.0/10 [00:00<00:00, 20.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fit the ALS model using the sparse item-user matrix\n",
    "model = implicit.als.AlternatingLeastSquares(factors=5, regularization=0.1, iterations=10) # hyperparameters to tune\n",
    "model.fit(sparse_item_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_recs(user, repeat_rec=False, n=2):\n",
    "    \"\"\"\n",
    "    Given user ID, uses Alternating Least Squares model and returns averaged item similarities \n",
    "    for all possible recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    user_code = dense_df['UserID'][dense_df['UserID']==user].cat.codes[0]\n",
    "    recs = model.recommend(user_code, sparse_user_item,  filter_already_liked_items=not repeat_rec, N=n)\n",
    "    recs = pd.DataFrame(recs, columns=['ItemID_code', 'Approximate Similarity'])\n",
    "    recs = pd.merge(dense_df[['ItemID', 'ItemID_code']], recs, how='inner', on=['ItemID_code'])\n",
    "    recs = recs.drop(['ItemID_code'], axis=1)\n",
    "\n",
    "    return recs.groupby(['ItemID']).mean().sort_values(by='Approximate Similarity', \n",
    "                                                                          ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two best new recommendations are returned for *User1* based on Alternating Least Squares matrix factorisation approach. <br><br>\n",
    "In the specific case below, there are only two available items that can be recommended to *User1*. Exact cosine similarities between these items and purchase history of *User1* are equal to 0. Matrix factorisation approximates this by yielding very low similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Approximate Similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item4</th>\n",
       "      <td>-0.001858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item2</th>\n",
       "      <td>-0.001858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Approximate Similarity\n",
       "ItemID                        \n",
       "item4                -0.001858\n",
       "item2                -0.001858"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_recs('User1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Recommendations\n",
    "Content-based recommender systems typically require a certain degree of insight into the data to gauge which content features are important. With just a handful of examples available, it is hard to evaluate this. Therefore, a generic approach to extracting semantics using a pre-trained model called [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf) (Cer at al., 2018) was chosen. The model uses Deep Averaging Network (DAN) model architecture to transform sentences and paragraphs into embedding vectors.<br><br>\n",
    "In addition to that, sentiment is extracted and used as a proxy for overall item quality. This is done using a simple rule-based general purpose sentiment analysis model called [VADER](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf) (Hutto & Gilbert, 2014). This approach is based on the assumtion that, despite the fact that reviews are subjective and user tastes may differ, if users are generally happy about the product, corresponding recommendation is more likely to be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting\n",
    "For the purpose of this excercise, it is assumed that review authors are unknown, but multiple reviews can be left for the same item. The data is converted to a Pandas dataframe.<br><br>\n",
    "Item 1:  this movie is really funny. It’s about war.<br>\n",
    "Item 2:  very sad! I hate it.<br>\n",
    "Item 3:  very bad movie. British is a nice place to live.<br>\n",
    "Item 4:  I have no idea about what this director want to say! Peace and war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = [['item1', 'this movie is really funny. It’s about war.'],\n",
    "              ['item2', 'very sad! I hate it.'],\n",
    "              ['item3', 'very bad movie. British is a nice place to live.'],\n",
    "              ['item4', 'I have no idea about what this director want to say! Peace and war.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item1</td>\n",
       "      <td>this movie is really funny. It’s about war.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item2</td>\n",
       "      <td>very sad! I hate it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item3</td>\n",
       "      <td>very bad movie. British is a nice place to live.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>item4</td>\n",
       "      <td>I have no idea about what this director want t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ItemID                                             Review\n",
       "0  item1        this movie is really funny. It’s about war.\n",
       "1  item2                               very sad! I hate it.\n",
       "2  item3   very bad movie. British is a nice place to live.\n",
       "3  item4  I have no idea about what this director want t..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.DataFrame(review_data, columns=['ItemID', 'Review'])\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantics-based recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Given a vector of texts, returns an array of embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.values.tolist()\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        embeddings = session.run(embed(text))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 16:52:58.798000 13428 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for all reviews\n",
    "embeddings = get_embeddings(reviews['Review'])\n",
    "emb_reviews = pd.concat([reviews, pd.DataFrame(embeddings)], axis=1)\n",
    "\n",
    "# calculate average embedding per item (if multiple reviews are available)\n",
    "emb_items = emb_reviews.groupby(['ItemID']).mean()\n",
    "\n",
    "# initialise cache\n",
    "review_cache = pd.DataFrame(columns=['Previously purchased item', 'Recommended item', 'Cosine Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_semantic(item):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between the item and all other items based on the review semantics\n",
    "    \"\"\"\n",
    "    S = np.dot(emb_items, emb_items.loc[item])\n",
    "    #return S\n",
    "    return pd.concat([pd.Series(np.resize(item, len(S))), pd.Series(emb_items.index), \n",
    "                      pd.Series(S)], axis=1, sort=False).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two best recommendations are returned for *User1* based on semantic content-based filtering approach. This is based on the assumption that we don't intend to recommend to repurchase the same items again.<br><br>\n",
    "In the specific case below, there are only two available items that can be recommended to *User1*. In contrast to collaborative filtering, here we can see that out of the two available movies, the one featuring the word 'war' in the review is ranked higher, as *User1* has already watched a war themed movie before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recommended item</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item4</th>\n",
       "      <td>0.285893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item2</th>\n",
       "      <td>0.202638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Cosine Similarity\n",
       "Recommended item                   \n",
       "item4                      0.285893\n",
       "item2                      0.202638"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bruteforce_recs('User1', sim_type='semantic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment-based recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Given a vector of texts, returns a vector of compound sentiment scores\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.values.tolist()\n",
    "    snts = []\n",
    "    for t in text:\n",
    "        snt = analyser.polarity_scores(t)['compound']\n",
    "        snts.append(snt)\n",
    "    return snts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentiment for all reviews\n",
    "sentiment = get_sentiment(reviews['Review'])\n",
    "sent_reviews = pd.concat([reviews, pd.DataFrame(sentiment, columns=['Sentiment'])], axis=1)\n",
    "\n",
    "# calculate average sentiment per item (if multiple reviews are available)\n",
    "sent_items = sent_reviews.groupby(['ItemID']).mean().sort_values(by='Sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_recs(user=None, repeat_rec=False, n=2):\n",
    "    \"\"\"\n",
    "    If non-repeat recomendations are required, returns top n items that have not been previously purchased by user.\n",
    "    Otherwise, returns top n recommendations by sentiment.\n",
    "    \"\"\"\n",
    "\n",
    "    unique_items = reviews['ItemID'].unique()\n",
    "    if repeat_rec == False:\n",
    "        previous_purchases = dataset[dataset['UserID']==user]['ItemID']\n",
    "        allowed_items = [x for x in unique_items if x not in previous_purchases.values]\n",
    "    else:\n",
    "        allowed_items = unique_items\n",
    "\n",
    "    return sent_items[sent_items.index.isin(allowed_items)].head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two new items with the most positive sentiment are returned for *User1*. This is based on the assumption that we don't intend to recommend to repurchase the same items again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item4</th>\n",
       "      <td>-0.3802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item2</th>\n",
       "      <td>-0.8254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment\n",
       "ItemID           \n",
       "item4     -0.3802\n",
       "item2     -0.8254"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_recs('User1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Recommender System\n",
    "Based on the performance of separate model components in different scenarios, a number of hybrid algorithms can be applied, such as:\n",
    "* weighted approach, where scores from each component are combined using a linear function\n",
    "* mixed approach, where top *n* recommendations from each component are combined into a single final list\n",
    "* switching approach, where certain component is preferred based on a specific scenario (e.g., movies with the highest sentiment are recommended to brand-new users with no purchase history, while collaborative filtering is used for everyone else)\n",
    "* enseble model, where best way of combining components determined by a machine learning approach<br>\n",
    "\n",
    "Here, due to limited amount of data available, a simple weighted approach with equal weights will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recs(user, repeat_rec=False, weights=[1, 1, 1], n=2):\n",
    "    \"\"\"\n",
    "    Given user ID and weights returns the result of a linear function of individual recommender components.\n",
    "    \"\"\"\n",
    "    \n",
    "    collaborative = bruteforce_recs(user, 'collaborative', repeat_rec, n=10)\n",
    "    semantic = bruteforce_recs(user, 'semantic', repeat_rec, n=10)\n",
    "    sentiment = sentiment_recs(user, repeat_rec, n=10)\n",
    "    \n",
    "    all_recs = pd.concat([collaborative, semantic, sentiment], axis=1, sort=True)\n",
    "    all_recs.columns = ['Collaborative Filtering', 'Content Filtering', 'Sentiment']\n",
    "    all_recs['Hybrid'] = weights[0] * all_recs['Collaborative Filtering'].values + \\\n",
    "                            weights[1] * all_recs['Content Filtering'].values + \\\n",
    "                            weights[2] * all_recs['Sentiment'].values\n",
    "    \n",
    "    return all_recs.sort_values(by='Hybrid', ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two new items with the highest combined score are returned for *User1*. This is based on the assumption that we don't intend to recommend to repurchase the same items again.<br><br>\n",
    "The combined recommendations allow to utilise all sources of information in ranking the recommendations. While none of the recommended items have been purchased by anyone who has a similar purchasing history as *User1*, the review for *item4* is semantically more similar to the reviews for items previously purchased by this user, and it is also more positive. Taken together, out of the two possible recommendations, *item4* is ranked higher than *item2*.<br><br>\n",
    "In real-world scenario it is unhighly likely that individual components have equal relevance, so the weighting should be tuned to reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collaborative Filtering</th>\n",
       "      <th>Content Filtering</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Hybrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285893</td>\n",
       "      <td>-0.3802</td>\n",
       "      <td>-0.094307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202638</td>\n",
       "      <td>-0.8254</td>\n",
       "      <td>-0.622762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Collaborative Filtering  Content Filtering  Sentiment    Hybrid\n",
       "item4                      0.0           0.285893    -0.3802 -0.094307\n",
       "item2                      0.0           0.202638    -0.8254 -0.622762"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_recs('User1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If repeat recommendations are allowed, we can see that sentiment component allows to differentiate between otherwise equivalent items - a movie with more positive overall review is ranked higher. Please note that repeat recommendations have not been penalised here in any way, so previous purchases are implicitly pushed to the top because of the high cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collaborative Filtering</th>\n",
       "      <th>Content Filtering</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Hybrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.769838</td>\n",
       "      <td>-0.1796</td>\n",
       "      <td>1.590238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.769837</td>\n",
       "      <td>-0.2484</td>\n",
       "      <td>1.521437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285893</td>\n",
       "      <td>-0.3802</td>\n",
       "      <td>-0.094307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202638</td>\n",
       "      <td>-0.8254</td>\n",
       "      <td>-0.622762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Collaborative Filtering  Content Filtering  Sentiment    Hybrid\n",
       "item1                      1.0           0.769838    -0.1796  1.590238\n",
       "item3                      1.0           0.769837    -0.2484  1.521437\n",
       "item4                      0.0           0.285893    -0.3802 -0.094307\n",
       "item2                      0.0           0.202638    -0.8254 -0.622762"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_recs('User1', repeat_rec=True, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "* Improve individual components - e.g., use more advanced sentiment model to extract sentiment that is related to the quality of the item (consider using the sentiment from the first, but not the second part of this review: *\"very bad movie. British is a nice place to live.\"*)\n",
    "* Improve efficiency of the algorithms by using approximate KNN search functions.\n",
    "* Build an ensemble model to combine all sources of data in the most optimal way.\n",
    "* Select the most optimal models and tune hyperparameters based on the model performance metrics.\n",
    "* Use an algorithm that extracts meaningful semantic features from reviews rather than using untransformed generic embeddings. E.g., reframe the algorithms as a supervised learning task to optimise the way information is extracted from reviews.\n",
    "* Combine review semantics and sentiment in a meaningful way - e.g., use negative sentiment as an indication that semantics used in a review are negatively assicoated with user's tastes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
